# End-to-End-RAG-System-Evaluation-with-Agentic-AI


# Study Title:
System Architecture, and Rollout with evaluation end-to-end Retrieval-Augmented Generation with Agentic AI

# System Overview:
End-to-end RAG project is completed in Jupyter Notebook with Anaconda Navigator. The project covers:  
•	Upload pdf paper,
•	 Make chunks from the paper
•	Do semantic embeddings
•	Use LLM for knowledge processing with relevant information
•	Analysis of outcomes precision and truthfulness
This notebook includes: user questions, real answers from paper, and artificial responses used for making an end to end RAG pipeline. Artificial responses are replaced with real responses from RAG.  This pipeline is applicable for Python ML/Generative AI, and Agentic AI. 

# Tech Stack:
•	LLM
•	Agentic AI
•	Prompt engineering
•	LangChain
•	RecursiveCharacterTextSplitter
•	Embedding model
•	DeepEval
•	Panda
•	Dataset created for RAG system
•	Evaluate RAG pipeline
•	Python jupyter Notebook

# Study Aim:
The objectives for a RAG pipeline easily replicated by others with quality analysis include:
•	Make an end-to-end RAG pipeline
•	Perform the pipeline
•	Use AI-generated answers set for RAG evaluation. 

# Critical Points:
•	Design RAG pipeline framework 
•	Pipeline supported with Agentic AI
•	Artificial data creation
•	RAG Accuracy and Faithfulness Scores: 51.8 % (Accuracy Score) ,  72.22 %  (Context Alignment)

# Summary of Findings:
End to end RAG system starting from document loading to LLM model evaluation is shown. The results tell the system is partially faithful. So, we need to find some space for improvement. In this project, I have shown my solid experience such as RAG system framework with NLP and LLM, Agentic AI systems, and Analytical evaluation pipeline. 

# Optimization of RAG Performance Metrics:
We can do the following things to get better scores. 
•	Play various chunks size and chunks overlap size. 
•	Find better embedding models  
•	Think of using a reranker model 
•	Need to refine structured prompt patterns
•	Force LLM to look at only retrieved text
•	Apply best k tuning to different chunks. 
•	Drop noisy chunks
•	Do chunk quality filtering
•	Use best LLM evaluator 

# Python Jupyter Notebook file:
end_to_end_RAG_evaluation_Agentic_AI.ipynb

